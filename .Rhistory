zh_anno <- left_join(zh_anno, urls, by = c("URL" = "url"))
zh_anno <- filter(zh_anno, spl == "in_sample")
zh_anno <- filter(zh_anno, keep != "n")
qbarplot(zh_anno, cxn, Hit) + guides(fill = guide_legend(title = "Variant")) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")
# re-import
texttype <- read_csv("texttypes.csv")
# bind with zh_anno
zh_anno <- left_join(zh_anno, texttype)
# convert response variable to factor
zh_anno$Hit <- factor(zh_anno$Hit)
zh_anno$cxn <- factor(zh_anno$cxn)
zh_anno$category <- factor(zh_anno$category)
zh_anno$texttype <- factor(zh_anno$texttype)
# get ctree
ct <- partykit::ctree(Hit ~ cxn + texttype, data = zh_anno)
cf <- cforest(Hit ~ cxn + texttype, data = zh_anno, controls = cforest_unbiased(mtry = 1))
# get variable importance
varimp <- varimp(cf, conditional = T)
# plot conditional inference tree
plot(ct, gp = gpar(fontsize = 12),
ep_args = list(justmin = 6),
ip_args=list(
abbreviate = FALSE))
1728/3
1080/3
rm(list=ls())
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# install "concordances" package, if not yet installed
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# install "wizard" package, if not yet installed
if(!is.element("wizard", installed.packages())) {
devtools::install_github("hartmast/wizard")
}
library(tidyverse)
library("koRpus.lang.de")
library(scales)
library(concordances)
library(pbapply)
library(readxl)
library(openxlsx)
library(kableExtra)
library(party)
library(lattice)
library(mlogit)
library(wizard)
# Chunk 2
# read data
zuhause <- read_csv("zuhause_lowercase.csv", col_types = c("d", rep("c", 7)))
Zuhause <- read_csv("Zuhause.csv")
zu_Hause <- read_csv("zu_Hause.csv", col_types = c("d", rep("c", 7)))
# add Month and Year columns
zuhause$Month <- gsub("^[0-9]{4}-|-[0-9]{2}$", "", zuhause$Date)
zuhause$Year <- gsub("-.*", "", zuhause$Date)
zuhause$YM   <- paste0(zuhause$Year, "-", zuhause$Month)
zu_Hause$Year <- gsub(".*\\.", "", zu_Hause$Date)
zu_Hause$Month <- gsub("^[0-9]{2}\\.|\\.[0-9]{4}$", "", zu_Hause$Date)
zu_Hause$YM  <- paste0(zu_Hause$Year, "-", zu_Hause$Month)
Zuhause$Month <- gsub("^[0-9]{4}-|-[0-9]{2}$", "", Zuhause$Date)
Zuhause$Year <- gsub("-.*", "", Zuhause$Date)
Zuhause$YM   <- paste0(Zuhause$Year, "-", Zuhause$Month)
# Chunk 3
# date as character (otherwise it won't bind)
zuhause$Date <- as.character(zuhause$Date)
# combine
zh <- rbind(mutate(zuhause, variant = "zuhause"),
mutate(Zuhause, variant = "Zuhause"),
mutate(zu_Hause, variant = "zu Hause"))
# Chunk 4
zh[which(zh$Hit=="zu"),]$Hit <- "zu Hause"
zh[which(zh$Hit=="zu Hause"),]$ContextAfter <- gsub("^Hause ", "", zh[which(zh$Hit=="zu Hause"),]$ContextAfter)
# Chunk 5
# Coronacorpus URLs
urls <- read_csv("coronakorpus_urls_all.csv")
# remove duplicates
urls <- urls[-which(duplicated(urls)),]
# detect URLs that are assigned to more than 1 category
urls_dup <- urls[which(duplicated(urls$url)),]$url
urls$duplicated <- character(nrow(urls))
urls[which(urls$url %in% urls_dup),]$duplicated <- "y"
urls[which(!urls$url %in% urls_dup),]$duplicated <- "n"
for(i in 1:length(urls_dup)) {
urls[which(urls$url==urls_dup[i]),]$category <- paste0(urls[which(urls$url==urls_dup[i]),]$category, collapse = "/")
}
# remove duplicates
urls <- urls[-which(duplicated(urls$url)),]
# join dataframes
zh <- left_join(zh, urls, by = c("URL" = "url"))
# Chunk 6
#export
# write_excel_csv(tibble(category = unique(zh$category),
#                        texttype = character(length(unique(zh$category)))),
#                 "texttypes.csv")
# re-import
texttype <- read_csv("texttypes.csv")
# join
zh <- left_join(zh, texttype)
# Chunk 7
# frequency of variant by Year/Month
zh1 <- zh %>% group_by(YM, variant) %>% summarise(
n = n()
)
# total frequency by Year/Month
zh2 <- zh %>% group_by(YM) %>% summarise(
n_all = n()
)
# combine both
zh_tbl <- left_join(zh1, zh2)
# add relative frequency
zh_tbl$rel <- zh_tbl$n / zh_tbl$n_all
# omit all before 2020
zh_tbl <- zh_tbl[grepl("2020|2021", zh_tbl$YM),]
# plot
zh_tbl %>% ggplot(aes (x = YM, y = rel, group = variant, col = variant)) +
geom_line(lwd = 1.2) + scale_y_continuous(labels = scales::percent) + theme_bw() + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Month") + ylab("Relative Frequency") +
guides(col = guide_legend(title = "Variant"))
# Chunk 8
qbarplot(zh, texttype, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Text type")
# Chunk 9
table(zh$Hit) %>% as.data.frame() %>% rename("Variant" = "Var1") %>% kbl()
# Chunk 10
zh$first_right <- first_right(zh, "ContextAfter", n = 1)
zh$last_left   <- last_left(zh, "ContextBefore", n = 1)
# Chunk 11
# get list of types in last_left and first_right columns
zh_last_left_first_right_types <- unique(c(zh$last_left, zh$first_right))
# run treetagger over those types
zh_last_left_first_right_types_tagged <- treetag(zh_last_left_first_right_types,
treetagger = "manual",
TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
preset = "de"), lang = "de", format = "obj")
# transform to dataframe
zh_last_left_first_right_types_tagged <- tibble(
token = as.character(zh_last_left_first_right_types_tagged@tokens$token),
pos   = as.character(zh_last_left_first_right_types_tagged@tokens$wclass)
)
# remove duplicates
if(any(duplicated(zh_last_left_first_right_types_tagged$token))) {
zh_last_left_first_right_types_tagged <- zh_last_left_first_right_types_tagged[-(which(duplicated(zh_last_left_first_right_types_tagged$token))),]
}
# rename column
zh_last_left_first_right_types_tagged <- rename(zh_last_left_first_right_types_tagged, c("LastLeftPOS" = "pos"))
# add to existing dataframe: left
zh <- left_join(zh, zh_last_left_first_right_types_tagged, by = c("last_left" = "token"), all.x = T)
# rename column
zh_last_left_first_right_types_tagged <- rename(zh_last_left_first_right_types_tagged, c("FirstRightPOS" = "LastLeftPOS"))
# add to existing dataframe: right
zh <- left_join(zh, zh_last_left_first_right_types_tagged, by = c("first_right" = "token"), all.x = T)
# Chunk 12
# new column: "cxn" for "construction"
zh$cxn <- character(nrow(zh))
# von zu Hause
zh[which(zh$last_left=="von"),]$cxn <- "von zh"
# zu Hause zu V-en
zh[which(zh$first_right=="zu" & zh$cxn == ""),]$cxn <- "zh zu v"
# zu Hause sein
zh[which(zh$first_right %in% c("sein", "bin", "bist", "ist", "seid", "sind", "war", "warst", "waren", "wart") | zh$last_left %in% c("sein", "bin", "bist", "ist", "seid", "sind", "war", "warst", "waren", "wart")),]$cxn <- "zh sein"
# zh einsperren / eingesperrt
zh[grep("ein(ge)?sperr.*", zh$last_left),]$cxn <- "zh EINSPERR"
zh[grep("ein(ge)?sperr.*", zh$first_right),]$cxn <- "zh EINSPERR"
# zh verbringen
zh[grep("verbring|verbrach", zh$first_right),]$cxn <- "zh verbringen"
zh[grep("verbring|verbrach", zh$last_left),]$cxn <- "zh verbringen"
# das (eigene) Zuhause
zh[which(zh$last_left %in% c("das", "dem", "ein", "einem")),]$cxn <- "ART zh"
zh[which(last_left(zh, "ContextBefore", n = 2) %in% c("das eigene", "dem eigenen", "ein eigenes", "einem eigenen")),]$cxn <- "ART eigene zh"
# PRONOUN (demonstrative/possessive & quantifiers) + Zuhause
zh[which(zh$last_left %in% c("Ihr", "ihr", "sein", "dein", "mein", "unser", "euer", "seinem", "unserem", "keinem", "dieses", "dessen", "Ihrem", "ihrem", "kein", "keinem", "manches", "mehrere", "meinem")),]$cxn <- "PRONOUN zh"
# zh in Quarantäne
zh[which(first_right(zh, "ContextAfter", n = 2) == "in Quarantäne"),]$cxn <- "zh in Quarantäne"
# für zh
zh[which(zh$last_left == "für"),]$cxn <- "für zh"
# zh arbeiten
zh[grep("arbeit.*", zh$last_left),]$cxn <- "zh arbeiten"
zh[grep("arbeit.*", zh$first_right),]$cxn <- "zh arbeiten"
# zh bleiben
zh[grep("[Bb]leib.*", last_left(zh, "ContextBefore", n = 5)),]$cxn = "zh bleiben"
zh[grep("[Bb]leib.*", first_right(zh, "ContextAfter", n = 5)),]$cxn = "zh bleiben"
# Chunk 13: spl
set.seed(1985)
spl <- sample(1:nrow(zh), 2000)
zh$spl <- character(nrow(zh))
zh[spl,]$spl <- "in_sample"
# zh %>% write.xlsx("zh_spl.xlsx", overwrite = T)
# Chunk 14
# write.xlsx(zh, "zh.xlsx")
# Chunk 15
zh_anno <- read_xlsx("zh_spl_anno.xlsx")
# Chunk 16
# fine-grained text type categories
zh_anno <- left_join(zh_anno, urls, by = c("URL" = "url"))
# coarse-grained text type categories
zh_anno <- left_join(zh_anno, texttype)
# Chunk 17
zh_anno <- filter(zh_anno, spl == "in_sample")
zh_anno <- filter(zh_anno, keep != "n")
# Chunk 18
qbarplot(zh_anno, cxn, Hit) + guides(fill = guide_legend(title = "Variant")) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")
# Chunk 19
# convert response variable to factor
zh_anno$Hit <- factor(zh_anno$Hit)
zh_anno$cxn <- factor(zh_anno$cxn)
zh_anno$category <- factor(zh_anno$category)
zh_anno$texttype <- factor(zh_anno$texttype)
# get ctree
ct <- partykit::ctree(Hit ~ cxn + texttype, data = zh_anno)
cf <- cforest(Hit ~ cxn + texttype, data = zh_anno, controls = cforest_unbiased(mtry = 1))
# get variable importance
varimp <- varimp(cf, conditional = T)
# plot conditional inference tree
plot(ct, gp = gpar(fontsize = 12),
ep_args = list(justmin = 6),
ip_args=list(
abbreviate = FALSE))
# Chunk 20
# random forest results: conditional variable importance
dotplot(varimp)
zh_anno
filter(zh_anno, cxn == "zh_V")
filter(zh_anno, cxn == "zh_V") %>% qbarplot(cxn_fine)
filter(zh_anno, cxn == "zh_V") %>% table(cxn_fine)
filter(zh_anno, cxn == "zh_V") %>% table(.$cxn_fine)
filter(zh_anno, cxn == "zh_V") %>% select(cxn_fine) %>% table
filter(zh_anno, cxn == "zh_V") %>% qbarplot(Hit, cxn_fine)
filter(zh_anno, cxn == "zh_V") %>% qbarplot(cxn_fine, Hit)
filter(zh_anno, cxn == "zh_V") %>% qbarplot(cxn_fine, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12))
zh_anno$cxn %>% unique
filter(zh_anno, cxn %in% c("zh_V", "zh_sein_bleiben")) %>% qbarplot(cxn_fine, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12))
filter(zh_anno, cxn %in% c("zh_V", "zh_sein_bleiben")) %>% qbarplot(cxn_fine, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")
?recode
?recode_factor
zh_anno$cxn
zh_anno$cxn_fine
zh_anno$cxn_fine %>% unique
zh_anno %>% mutate(cxn_fine2 = recode(cxn_fine, "zh in Quarantäne" = "zh V", "zh EINSPERR" = "zh V"))
zh_anno %>% mutate(cxn_fine2 = recode(cxn_fine, "zh in Quarantäne" = "zh V", "zh EINSPERR" = "zh V")) %>% filter(cxn %in% c("zh_V", "zh_sein_bleiben"))
zh_anno %>% mutate(cxn_fine2 = recode(cxn_fine, "zh in Quarantäne" = "zh V", "zh EINSPERR" = "zh V")) %>% filter(cxn %in% c("zh_V", "zh_sein_bleiben")) %>% qbarplot(cxn_fine, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")
zh_anno %>% mutate(cxn_fine2 = recode(cxn_fine, "zh in Quarantäne" = "zh V", "zh EINSPERR" = "zh V")) %>% filter(cxn %in% c("zh_V", "zh_sein_bleiben")) %>% qbarplot(cxn_fine2, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")
zh_anno2 <- filter(zh_anno, cxn != "zh_N")
zh_anno2$cxn <- droplevels(zh_anno2$cxn)
mutate(zh_anno, case = ifelse(Hit %in% c("zu Hause", "zuhause"), "lower", "upper"))
mutate(zh_anno, case = ifelse(Hit %in% c("zu Hause", "zuhause"), "lower", "upper"), spelling = ifelse(Hit %in% c("zuhause", "Zuhause"), "solid", "open"))
mutate(zh_anno2, case = ifelse(Hit %in% c("zu Hause", "zuhause"), "lower", "upper"), spelling = ifelse(Hit %in% c("zuhause", "Zuhause"), "solid", "open"))
zh_anno2 <- mutate(zh_anno2, case = ifelse(Hit %in% c("zu Hause", "zuhause"), "lower", "upper"), spelling = ifelse(Hit %in% c("zuhause", "Zuhause"), "solid", "open"))
library(lme4)
zh_anno2$Bibl
zh_anno2$Genre
zh_anno2$URL
# add top-level domain so that it can be used as random effect
gsub("/.*", "", zh_anno2$URL[1])
# add top-level domain so that it can be used as random effect
gsub("(?<=/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<=:/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<=/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?!=/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?!=:/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<!:/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<!:)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<!:/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<!:/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<!\\:/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<!\\:\\/)/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<![:/])/.*", "", zh_anno2$URL[1], perl = T)
# add top-level domain so that it can be used as random effect
gsub("(?<![:/])/.*", "", zh_anno2$URL, perl = T)
# add top-level domain so that it can be used as random effect
mutate(zh_anno2, toplevel_domain = gsub("(?<![:/])/.*", "", zh_anno2$URL, perl = T))
# add top-level domain so that it can be used as random effect
zh_anno2 <- mutate(zh_anno2, toplevel_domain = gsub("(?<![:/])/.*", "", zh_anno2$URL, perl = T))
# model: open vs. solid spelling
glmer(spelling ~ cxn + (1 | toplevel_domain), family = "binomial")
# model: open vs. solid spelling
m <- glmer(spelling ~ cxn + (1 | toplevel_domain), family = "binomial", data = zh_anno2)
# model: open vs. solid spelling
m <- glmer(factor(spelling) ~ cxn + (1 | toplevel_domain), family = "binomial", data = zh_anno2)
summary(m)
# model: open vs. solid spelling
m <- glm(factor(spelling) ~ cxn + texttype, family = "binomial", data = zh_anno2)
summary(m)
# model: open vs. solid spelling
m <- glm(factor(spelling) ~ cxn + (1 | texttype), family = "binomial", data = zh_anno2)
# model: open vs. solid spelling
m <- glmer(factor(spelling) ~ cxn + (1 | texttype), family = "binomial", data = zh_anno2)
# model: open vs. solid spelling
m <- glmer(factor(spelling) ~ cxn + (1 | toplevel_domain), family = "binomial", data = zh_anno2)
summary(m)
m <- glmer(factor(case) ~ cxn + (1 | toplevel_domain), family = "binomial", data = zh_anno2)
summary(m)
rm(list=ls())
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# install "concordances" package, if not yet installed
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# install "wizard" package, if not yet installed
if(!is.element("wizard", installed.packages())) {
devtools::install_github("hartmast/wizard")
}
library(tidyverse)
library("koRpus.lang.de")
library(scales)
library(concordances)
library(pbapply)
library(readxl)
library(openxlsx)
library(kableExtra)
library(party)
library(lattice)
library(mlogit)
library(wizard)
library(lme4)
# Chunk 2
# read data
zuhause <- read_csv("zuhause_lowercase.csv", col_types = c("d", rep("c", 7)))
Zuhause <- read_csv("Zuhause.csv")
zu_Hause <- read_csv("zu_Hause.csv", col_types = c("d", rep("c", 7)))
# add Month and Year columns
zuhause$Month <- gsub("^[0-9]{4}-|-[0-9]{2}$", "", zuhause$Date)
zuhause$Year <- gsub("-.*", "", zuhause$Date)
zuhause$YM   <- paste0(zuhause$Year, "-", zuhause$Month)
zu_Hause$Year <- gsub(".*\\.", "", zu_Hause$Date)
zu_Hause$Month <- gsub("^[0-9]{2}\\.|\\.[0-9]{4}$", "", zu_Hause$Date)
zu_Hause$YM  <- paste0(zu_Hause$Year, "-", zu_Hause$Month)
Zuhause$Month <- gsub("^[0-9]{4}-|-[0-9]{2}$", "", Zuhause$Date)
Zuhause$Year <- gsub("-.*", "", Zuhause$Date)
Zuhause$YM   <- paste0(Zuhause$Year, "-", Zuhause$Month)
# Chunk 3
# date as character (otherwise it won't bind)
zuhause$Date <- as.character(zuhause$Date)
# combine
zh <- rbind(mutate(zuhause, variant = "zuhause"),
mutate(Zuhause, variant = "Zuhause"),
mutate(zu_Hause, variant = "zu Hause"))
# Chunk 4
zh[which(zh$Hit=="zu"),]$Hit <- "zu Hause"
zh[which(zh$Hit=="zu Hause"),]$ContextAfter <- gsub("^Hause ", "", zh[which(zh$Hit=="zu Hause"),]$ContextAfter)
# Chunk 5
# Coronacorpus URLs
urls <- read_csv("coronakorpus_urls_all.csv")
# remove duplicates
urls <- urls[-which(duplicated(urls)),]
# detect URLs that are assigned to more than 1 category
urls_dup <- urls[which(duplicated(urls$url)),]$url
urls$duplicated <- character(nrow(urls))
urls[which(urls$url %in% urls_dup),]$duplicated <- "y"
urls[which(!urls$url %in% urls_dup),]$duplicated <- "n"
for(i in 1:length(urls_dup)) {
urls[which(urls$url==urls_dup[i]),]$category <- paste0(urls[which(urls$url==urls_dup[i]),]$category, collapse = "/")
}
# remove duplicates
urls <- urls[-which(duplicated(urls$url)),]
# join dataframes
zh <- left_join(zh, urls, by = c("URL" = "url"))
# Chunk 6
#export
# write_excel_csv(tibble(category = unique(zh$category),
#                        texttype = character(length(unique(zh$category)))),
#                 "texttypes.csv")
# re-import
texttype <- read_csv("texttypes.csv")
# join
zh <- left_join(zh, texttype)
# Chunk 7
# frequency of variant by Year/Month
zh1 <- zh %>% group_by(YM, variant) %>% summarise(
n = n()
)
# total frequency by Year/Month
zh2 <- zh %>% group_by(YM) %>% summarise(
n_all = n()
)
# combine both
zh_tbl <- left_join(zh1, zh2)
# add relative frequency
zh_tbl$rel <- zh_tbl$n / zh_tbl$n_all
# omit all before 2020
zh_tbl <- zh_tbl[grepl("2020|2021", zh_tbl$YM),]
# plot
zh_tbl %>% ggplot(aes (x = YM, y = rel, group = variant, col = variant)) +
geom_line(lwd = 1.2) + scale_y_continuous(labels = scales::percent) + theme_bw() + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Month") + ylab("Relative Frequency") +
guides(col = guide_legend(title = "Variant"))
# Chunk 8
qbarplot(zh, texttype, Hit) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Text type")
# Chunk 9
table(zh$Hit) %>% as.data.frame() %>% rename("Variant" = "Var1") %>% kbl()
# Chunk 10
zh$first_right <- first_right(zh, "ContextAfter", n = 1)
zh$last_left   <- last_left(zh, "ContextBefore", n = 1)
# Chunk 11
# get list of types in last_left and first_right columns
zh_last_left_first_right_types <- unique(c(zh$last_left, zh$first_right))
# run treetagger over those types
zh_last_left_first_right_types_tagged <- treetag(zh_last_left_first_right_types,
treetagger = "manual",
TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
preset = "de"), lang = "de", format = "obj")
# transform to dataframe
zh_last_left_first_right_types_tagged <- tibble(
token = as.character(zh_last_left_first_right_types_tagged@tokens$token),
pos   = as.character(zh_last_left_first_right_types_tagged@tokens$wclass)
)
# remove duplicates
if(any(duplicated(zh_last_left_first_right_types_tagged$token))) {
zh_last_left_first_right_types_tagged <- zh_last_left_first_right_types_tagged[-(which(duplicated(zh_last_left_first_right_types_tagged$token))),]
}
# rename column
zh_last_left_first_right_types_tagged <- rename(zh_last_left_first_right_types_tagged, c("LastLeftPOS" = "pos"))
# add to existing dataframe: left
zh <- left_join(zh, zh_last_left_first_right_types_tagged, by = c("last_left" = "token"), all.x = T)
# rename column
zh_last_left_first_right_types_tagged <- rename(zh_last_left_first_right_types_tagged, c("FirstRightPOS" = "LastLeftPOS"))
# add to existing dataframe: right
zh <- left_join(zh, zh_last_left_first_right_types_tagged, by = c("first_right" = "token"), all.x = T)
# Chunk 12
# new column: "cxn" for "construction"
zh$cxn <- character(nrow(zh))
# von zu Hause
zh[which(zh$last_left=="von"),]$cxn <- "von zh"
# zu Hause zu V-en
zh[which(zh$first_right=="zu" & zh$cxn == ""),]$cxn <- "zh zu v"
# zu Hause sein
zh[which(zh$first_right %in% c("sein", "bin", "bist", "ist", "seid", "sind", "war", "warst", "waren", "wart") | zh$last_left %in% c("sein", "bin", "bist", "ist", "seid", "sind", "war", "warst", "waren", "wart")),]$cxn <- "zh sein"
# zh einsperren / eingesperrt
zh[grep("ein(ge)?sperr.*", zh$last_left),]$cxn <- "zh EINSPERR"
zh[grep("ein(ge)?sperr.*", zh$first_right),]$cxn <- "zh EINSPERR"
# zh verbringen
zh[grep("verbring|verbrach", zh$first_right),]$cxn <- "zh verbringen"
zh[grep("verbring|verbrach", zh$last_left),]$cxn <- "zh verbringen"
# das (eigene) Zuhause
zh[which(zh$last_left %in% c("das", "dem", "ein", "einem")),]$cxn <- "ART zh"
zh[which(last_left(zh, "ContextBefore", n = 2) %in% c("das eigene", "dem eigenen", "ein eigenes", "einem eigenen")),]$cxn <- "ART eigene zh"
# PRONOUN (demonstrative/possessive & quantifiers) + Zuhause
zh[which(zh$last_left %in% c("Ihr", "ihr", "sein", "dein", "mein", "unser", "euer", "seinem", "unserem", "keinem", "dieses", "dessen", "Ihrem", "ihrem", "kein", "keinem", "manches", "mehrere", "meinem")),]$cxn <- "PRONOUN zh"
# zh in Quarantäne
zh[which(first_right(zh, "ContextAfter", n = 2) == "in Quarantäne"),]$cxn <- "zh in Quarantäne"
# für zh
zh[which(zh$last_left == "für"),]$cxn <- "für zh"
# zh arbeiten
zh[grep("arbeit.*", zh$last_left),]$cxn <- "zh arbeiten"
zh[grep("arbeit.*", zh$first_right),]$cxn <- "zh arbeiten"
# zh bleiben
zh[grep("[Bb]leib.*", last_left(zh, "ContextBefore", n = 5)),]$cxn = "zh bleiben"
zh[grep("[Bb]leib.*", first_right(zh, "ContextAfter", n = 5)),]$cxn = "zh bleiben"
# Chunk 13: spl
set.seed(1985)
spl <- sample(1:nrow(zh), 2000)
zh$spl <- character(nrow(zh))
zh[spl,]$spl <- "in_sample"
# zh %>% write.xlsx("zh_spl.xlsx", overwrite = T)
# Chunk 14
# write.xlsx(zh, "zh.xlsx")
# Chunk 15
zh_anno <- read_xlsx("zh_spl_anno.xlsx")
# Chunk 16
# fine-grained text type categories
zh_anno <- left_join(zh_anno, urls, by = c("URL" = "url"))
# coarse-grained text type categories
zh_anno <- left_join(zh_anno, texttype)
# Chunk 17
zh_anno <- filter(zh_anno, spl == "in_sample")
zh_anno <- filter(zh_anno, keep != "n")
# Chunk 18
qbarplot(zh_anno, cxn, Hit) + guides(fill = guide_legend(title = "Variant")) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")
# Chunk 19
# convert response variable to factor
zh_anno$Hit <- factor(zh_anno$Hit)
zh_anno$cxn <- factor(zh_anno$cxn)
zh_anno$category <- factor(zh_anno$category)
zh_anno$texttype <- factor(zh_anno$texttype)
# get ctree
ct <- partykit::ctree(Hit ~ cxn + texttype, data = zh_anno)
cf <- cforest(Hit ~ cxn + texttype, data = zh_anno, controls = cforest_unbiased(mtry = 1))
# get variable importance
varimp <- varimp(cf, conditional = T)
# plot conditional inference tree
plot(ct, gp = gpar(fontsize = 12),
ep_args = list(justmin = 6),
ip_args=list(
abbreviate = FALSE))
# random forest results: conditional variable importance
dotplot(varimp)
?dotplot
# random forest results: conditional variable importance
dotplot(varimp, cex=12)
# random forest results: conditional variable importance
dotplot(varimp, cex=10)
# random forest results: conditional variable importance
dotplot(varimp, cex=2)
# random forest results: conditional variable importance
dotplot(varimp, cex=2, cex.label = 2)
# random forest results: conditional variable importance
dotplot(varimp, cex=2, cex.axis = 2)
?cex
# random forest results: conditional variable importance
dotplot(varimp, cex=2, scales = list(cex=2))
255*3
