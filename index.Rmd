---
title: "zu Hause / zuhause / Zuhause"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
    collapsed: false
    df_print: paged
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# install "concordances" package, if not yet installed
if(!is.element("concordances", installed.packages())) {
    devtools::install_github("hartmast/concordances")
}

# install "wizard" package, if not yet installed
if(!is.element("wizard", installed.packages())) {
    devtools::install_github("hartmast/wizard")
}

library(tidyverse)
library("koRpus.lang.de")
library(scales)
library(concordances)
library(pbapply)
library(readxl)
library(openxlsx)
library(kableExtra)
library(party)
library(lattice)
library(mlogit)
library(wizard)

```

# About this document

This document contains supplementary material to a study on graphemic variation in preposition-noun combinations, focusing on the example of *zu Hause / zuhause / Zuhause*.


## Data collection

Barbaresi's (2021) [Corona corpus](https://github.com/adbar/coronakorpus) was used as the term can be expected to occur very frequently in those data. The corpus was queried via [DWDS](https://dwds.de) for instances of `@zu @Hause`, `@zuhause`, and `@Zuhause` (in the DWDS query syntax, the @ operator is used to search for exact word forms). The queries were performed on August 28, 2021.


## Data wrangling

First, we read in the data:

```{r, message=FALSE}

# read data
zuhause <- read_csv("zuhause_lowercase.csv", col_types = c("d", rep("c", 7)))
Zuhause <- read_csv("Zuhause.csv")
zu_Hause <- read_csv("zu_Hause.csv", col_types = c("d", rep("c", 7)))

# add Month and Year columns
zuhause$Month <- gsub("^[0-9]{4}-|-[0-9]{2}$", "", zuhause$Date)
zuhause$Year <- gsub("-.*", "", zuhause$Date)
zuhause$YM   <- paste0(zuhause$Year, "-", zuhause$Month)

zu_Hause$Year <- gsub(".*\\.", "", zu_Hause$Date)
zu_Hause$Month <- gsub("^[0-9]{2}\\.|\\.[0-9]{4}$", "", zu_Hause$Date)
zu_Hause$YM  <- paste0(zu_Hause$Year, "-", zu_Hause$Month)


Zuhause$Month <- gsub("^[0-9]{4}-|-[0-9]{2}$", "", Zuhause$Date)
Zuhause$Year <- gsub("-.*", "", Zuhause$Date)
Zuhause$YM   <- paste0(Zuhause$Year, "-", Zuhause$Month)

```

Combining the three variants in one table

```{r}

# date as character (otherwise it won't bind)
zuhause$Date <- as.character(zuhause$Date)

# combine
zh <- rbind(mutate(zuhause, variant = "zuhause"),
      mutate(Zuhause, variant = "Zuhause"),
      mutate(zu_Hause, variant = "zu Hause"))


```

For "zu Hause", we move "Hause" in the keyword column

```{r}

zh[which(zh$Hit=="zu"),]$Hit <- "zu Hause"
zh[which(zh$Hit=="zu Hause"),]$ContextAfter <- gsub("^Hause ", "", zh[which(zh$Hit=="zu Hause"),]$ContextAfter)

```

For a deeper analysis of the different text types involved, we add metadata from the Corona corpus URL collection (available at https://github.com/adbar/coronakorpus)

```{r}

# Coronacorpus URLs
urls <- read_csv("coronakorpus_urls_all.csv")

# remove duplicates
urls <- urls[-which(duplicated(urls)),]

# detect URLs that are assigned to more than 1 category
urls_dup <- urls[which(duplicated(urls$url)),]$url
urls$duplicated <- character(nrow(urls))
urls[which(urls$url %in% urls_dup),]$duplicated <- "y"
urls[which(!urls$url %in% urls_dup),]$duplicated <- "n"

for(i in 1:length(urls_dup)) {
  urls[which(urls$url==urls_dup[i]),]$category <- paste0(urls[which(urls$url==urls_dup[i]),]$category, collapse = "/")
}

# remove duplicates
urls <- urls[-which(duplicated(urls$url)),]


# join dataframes
zh <- left_join(zh, urls, by = c("URL" = "url"))

```


Frequency of each spelling variant

```{r}

# frequency of variant by Year/Month
zh1 <- zh %>% group_by(YM, variant) %>% summarise(
  n = n()
)

# total frequency by Year/Month
zh2 <- zh %>% group_by(YM) %>% summarise(
  n_all = n()
)

# combine both
zh_tbl <- left_join(zh1, zh2)

# add relative frequency
zh_tbl$rel <- zh_tbl$n / zh_tbl$n_all

# omit all before 2020
zh_tbl <- zh_tbl[grepl("2020|2021", zh_tbl$YM),]

# plot
zh_tbl %>% ggplot(aes (x = YM, y = rel, group = variant, col = variant)) +
  geom_line(lwd = 1.2) + scale_y_continuous(labels = scales::percent) + theme_bw() + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Month") + ylab("Relative Frequency") + 
  guides(col = guide_legend(title = "Variant"))

```

Frequency in tabular form

```{r}

table(zh$Hit) %>% as.data.frame() %>% rename("Variant" = "Var1") %>% kbl()

```


get last word on the left and first on the right

```{r}

zh$first_right <- first_right(zh, "ContextAfter", n = 1)
zh$last_left   <- last_left(zh, "ContextBefore", n = 1)

```

This allows us to automatically identify some frequent patterns, e.g. "zu Hause bleiben" etc. For this purpose, it can help to identify the part-of-speech of the context words.

```{r}

# get list of types in last_left and first_right columns
zh_last_left_first_right_types <- unique(c(zh$last_left, zh$first_right))


# run treetagger over those types
zh_last_left_first_right_types_tagged <- treetag(zh_last_left_first_right_types,
                                                   treetagger = "manual",
              TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
                                preset = "de"), lang = "de", format = "obj")


# transform to dataframe
zh_last_left_first_right_types_tagged <- tibble(
  token = as.character(zh_last_left_first_right_types_tagged@tokens$token),
  pos   = as.character(zh_last_left_first_right_types_tagged@tokens$wclass)
)

# remove duplicates
if(any(duplicated(zh_last_left_first_right_types_tagged$token))) {
  zh_last_left_first_right_types_tagged <- zh_last_left_first_right_types_tagged[-(which(duplicated(zh_last_left_first_right_types_tagged$token))),]
}

# rename column
zh_last_left_first_right_types_tagged <- rename(zh_last_left_first_right_types_tagged, c("LastLeftPOS" = "pos"))

# add to existing dataframe: left
zh <- left_join(zh, zh_last_left_first_right_types_tagged, by = c("last_left" = "token"), all.x = T)

# rename column
zh_last_left_first_right_types_tagged <- rename(zh_last_left_first_right_types_tagged, c("FirstRightPOS" = "LastLeftPOS"))

# add to existing dataframe: right
zh <- left_join(zh, zh_last_left_first_right_types_tagged, by = c("first_right" = "token"), all.x = T)



# export left andright context for tagging

# pbsapply(1:nrow(zh), function(i) write.table(zh$ContextBefore[i], file = paste0("tmp/", i, ".txt"), quote = F, row.names = F, col.names = F))
# 


# pbsapply(1:10, function(i) paste0(as.character(treetag(zh$ContextBefore[i],
#               treetagger = "manual",
#               TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
#                                 preset = "de"), lang = "de", format = "obj")@tokens$wclass), collapse = " "))


# paste0(as.character(treetag(zh$ContextBefore[1],
#               treetagger = "manual", 
#               TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
#                                 preset = "de"), lang = "de", format = "obj")@tokens$wclass), collapse = " ")

# pbsapply(1:nrow(zh), function(i) treetag(zh$last_left[i],
#               treetagger = "manual", 
#               TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
#                                 preset = "de"), lang = "de", format = "obj")@tokens$wclass)


# zh_left_tags <- treetag(zh$last_left,
#               treetagger = "manual", 
#               TT.options = list(path = "/Users/stefanhartmann/Downloads/TreeTagger/",
#                                 preset = "de"), lang = "de", format = "obj")



```


```{r}

# new column: "cxn" for "construction"
zh$cxn <- character(nrow(zh))

# von zu Hause
zh[which(zh$last_left=="von"),]$cxn <- "von zh"

# zu Hause zu V-en
zh[which(zh$first_right=="zu" & zh$cxn == ""),]$cxn <- "zh zu v"

# zu Hause sein
zh[which(zh$first_right %in% c("sein", "bin", "bist", "ist", "seid", "sind", "war", "warst", "waren", "wart") | zh$last_left %in% c("sein", "bin", "bist", "ist", "seid", "sind", "war", "warst", "waren", "wart")),]$cxn <- "zh sein"

# zh einsperren / eingesperrt
zh[grep("ein(ge)?sperr.*", zh$last_left),]$cxn <- "zh EINSPERR"
zh[grep("ein(ge)?sperr.*", zh$first_right),]$cxn <- "zh EINSPERR"

# zh verbringen
zh[grep("verbring|verbrach", zh$first_right),]$cxn <- "zh verbringen"
zh[grep("verbring|verbrach", zh$last_left),]$cxn <- "zh verbringen"

# das (eigene) Zuhause
zh[which(zh$last_left %in% c("das", "dem", "ein", "einem")),]$cxn <- "ART zh"
zh[which(last_left(zh, "ContextBefore", n = 2) %in% c("das eigene", "dem eigenen", "ein eigenes", "einem eigenen")),]$cxn <- "ART eigene zh"

# PRONOUN (demonstrative/possessive & quantifiers) + Zuhause
zh[which(zh$last_left %in% c("Ihr", "ihr", "sein", "dein", "mein", "unser", "euer", "seinem", "unserem", "keinem", "dieses", "dessen", "Ihrem", "ihrem", "kein", "keinem", "manches", "mehrere", "meinem")),]$cxn <- "PRONOUN zh"

# zh in Quarantäne
zh[which(first_right(zh, "ContextAfter", n = 2) == "in Quarantäne"),]$cxn <- "zh in Quarantäne"

# für zh
zh[which(zh$last_left == "für"),]$cxn <- "für zh"

# zh arbeiten
zh[grep("arbeit.*", zh$last_left),]$cxn <- "zh arbeiten"
zh[grep("arbeit.*", zh$first_right),]$cxn <- "zh arbeiten"


# zh bleiben
zh[grep("[Bb]leib.*", last_left(zh, "ContextBefore", n = 5)),]$cxn = "zh bleiben"
zh[grep("[Bb]leib.*", first_right(zh, "ContextAfter", n = 5)),]$cxn = "zh bleiben"

```

## Annotated sample

Get a sample for manual annotation

```{r spl}

set.seed(1985)
spl <- sample(1:nrow(zh), 2000)
zh$spl <- character(nrow(zh))
zh[spl,]$spl <- "in_sample"
# zh %>% write.xlsx("zh_spl.xlsx", overwrite = T)

```

Export for further annotation

```{r}

# write.xlsx(zh, "zh.xlsx")

```

Read file with annotated sample

```{r}

zh_anno <- read_xlsx("zh_spl_anno.xlsx")

```

Add category information (again)

```{r}

zh_anno <- left_join(zh_anno, urls, by = c("URL" = "url"))

```

Filter: only sample; remove false hits

```{r}

zh_anno <- filter(zh_anno, spl == "in_sample")
zh_anno <- filter(zh_anno, keep != "n")

```

## Frequency plot

Simple table

```{r}

qbarplot(zh_anno, cxn, Hit) + guides(fill = guide_legend(title = "Variant")) + theme(axis.text.x = element_text(angle=45, hjust=.9, size=12)) + xlab("Construction")

```

Export list of text type categories for simplifying them manually

```{r}

#export

# write_excel_csv(tibble(category = unique(zh$category),
#                        texttype = character(length(unique(zh$category)))),
#                 "texttypes.csv")

# re-import
texttype <- read_csv("texttypes.csv")

# bind with zh_anno
zh_anno <- left_join(zh_anno, texttype)


```

Conditional inference tree

```{r fig.height=15, fig.width=20}

# convert response variable to factor
zh_anno$Hit <- factor(zh_anno$Hit)
zh_anno$cxn <- factor(zh_anno$cxn)
zh_anno$category <- factor(zh_anno$category)
zh_anno$texttype <- factor(zh_anno$texttype)

# get ctree
ct <- partykit::ctree(Hit ~ cxn + texttype, data = zh_anno)
cf <- cforest(Hit ~ cxn + texttype, data = zh_anno, controls = cforest_unbiased(mtry = 1))

# get variable importance
varimp <- varimp(cf, conditional = T)

# plot conditional inference tree
plot(ct, gp = gpar(fontsize = 12), 
       ep_args = list(justmin = 6),
  ip_args=list(
       abbreviate = TRUE))


# random forest results: conditional variable importance
dotplot(varimp)



```

